{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_word2vec_sif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atomnp/realtime_text_similarity_backend/blob/main/tfidf_word2vec_sif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "CxExVdm3LZrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "HeRbR-2NAvAS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import coo_matrix, lil_matrix\n",
        "import numpy as np\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wysvPmYtEjY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0518a3ba-da3d-499c-ff55-5330cb27638d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=[\"?\",\"n't\",\"'s\"]\n",
        "stop_words+=nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "CkcMya7t8CA3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting your google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xloBLoD9FfKw",
        "outputId": "99fde67a-47a9-4cf5-891a-3efcae494b45"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the data"
      ],
      "metadata": {
        "id": "3KOwZpxvIimd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make shortcut of [this](https://drive.google.com/drive/folders/1BGr0cWKiJwT_jNg9nRNAhWgy0mYPgw_K?usp=sharing) folder in your gdrive**"
      ],
      "metadata": {
        "id": "cJnXuBlZG23A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = r'/gdrive/MyDrive/minor_project_files/filtered.txt'\n",
        "\n",
        "# df= pd.read_csv(dataset, keep_default_na=False, na_values=['_'])\n",
        "questions = []\n",
        "with open(dataset,\"r\") as fp:\n",
        "  questions=[x.strip().lower() for x in fp.readlines()]"
      ],
      "metadata": {
        "id": "42wJ0pffSiRN"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize sentences\n",
        "sentences = (word_tokenize(sentence) for sentence in questions)"
      ],
      "metadata": {
        "id": "ysCGVpAeOUbE"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec Model (Train, Save and Load)"
      ],
      "metadata": {
        "id": "mWALbqWcFBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "class callback(CallbackAny2Vec):\n",
        "    '''Callback to print loss after each epoch.'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "      loss = model.get_latest_training_loss()\n",
        "      if self.epoch == 0:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "      else:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "      self.epoch += 1\n",
        "      self.loss_previous_step = loss"
      ],
      "metadata": {
        "id": "2sNEI7X9M3di"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if you want to retrain the word2vec model\n",
        "\n",
        "it_copy, sentences = itertools.tee(sentences)\n",
        "\n",
        "# min_count, window=3?? !!\n",
        "model = Word2Vec(sentences=list(it_copy), size=100, window=5, min_count=1, workers=4, compute_loss=True, iter=6, callbacks=[callback()])\n",
        "model_wv = model.wv\n",
        "model.save(\"/gdrive/MyDrive/minor_project_files/word2vec.model\")\n",
        "# # model = FastText( window=3, min_count=1)  # instantiate\n",
        "# # model.build_vocab(sentences=it_copy)\n",
        "# # model.train(sentences=it_copy, total_examples=len(questions), epochs=10)  # train\n",
        "# # model.save(\"/gdrive/MyDrive/minor_project_files/fasttext.model\")"
      ],
      "metadata": {
        "id": "GDD83pHX8FaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804507a3-4206-467c-b7f5-bed1c5638fac"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 1719031.25\n",
            "Loss after epoch 1: 1352354.5\n",
            "Loss after epoch 2: 1235897.25\n",
            "Loss after epoch 3: 1051952.5\n",
            "Loss after epoch 4: 997084.0\n",
            "Loss after epoch 5: 1021777.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load already saved word2vec model\n",
        "# model = FastText.load(\"/gdrive/MyDrive/minor_project_files/fasttext.model\")\n",
        "model = Word2Vec.load(\"/gdrive/MyDrive/minor_project_files/word2vec.model\")\n",
        "model_wv = model.wv"
      ],
      "metadata": {
        "id": "6N1UnBrUORUe"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('china')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgkzTCEtGQRn",
        "outputId": "e94e68db-7408-4a0f-eef3-84779c64cf35"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('pakistan', 0.8222455978393555),\n",
              " ('russia', 0.8220350742340088),\n",
              " ('israel', 0.8166234493255615),\n",
              " ('taiwan', 0.8009499311447144),\n",
              " ('iran', 0.7991870641708374),\n",
              " ('japan', 0.7957135438919067),\n",
              " ('america', 0.7903860211372375),\n",
              " ('africa', 0.7748715281486511),\n",
              " ('bangladesh', 0.7601543664932251),\n",
              " ('nepal', 0.7534977793693542)]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^^ First train the model on the entire dataset"
      ],
      "metadata": {
        "id": "Y_gDCmUi3T32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TFIDF for finding important words in a sentence"
      ],
      "metadata": {
        "id": "efMdPLqbHuPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "      return text\n",
        "\n",
        "# lowercase !!\n",
        "vect = TfidfVectorizer(stop_words=stop_words, use_idf=True, tokenizer=identity_tokenizer,lowercase=False)    \n",
        "# copy the iterator so that the cell can be rerun (otherwise the iterator will be at the end)\n",
        "it_copy, sentences = itertools.tee(sentences)\n",
        "tfidf_matrix = vect.fit_transform(it_copy)\n",
        "fv = vect.get_feature_names()"
      ],
      "metadata": {
        "id": "qK8-4oYe2Xb_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation to find sentence embeddings (alternative 1) : lil_matrix! 2m4s!!\n",
        "\n",
        "# cx = coo_matrix(tfidf_matrix)\n",
        "cx = lil_matrix(tfidf_matrix)"
      ],
      "metadata": {
        "id": "HXMKv8ZE49dt"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find sentence Embeddings for each sentence in the dataset\n",
        "\n",
        "\n",
        "1.   Find most important words\n",
        "2.   Lookup their word vectors from word2vec/glove model\n",
        "3.   Save the average word vector as the sentence embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "ehay95iFH0o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# todo: get a proper word frequency for a word in a document set\n",
        "# or perhaps just a typical frequency for a word from Google's n-grams\n",
        "def get_word_frequency(word_text):\n",
        "    return 0.0001  # set to a low occurring frequency - probably not unrealistic for most words, improves vector values\n"
      ],
      "metadata": {
        "id": "Td3Aqf6KJA_g"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted(arrlist, a: float = 1e-3):\n",
        "    # print(arrlist.shape)\n",
        "    vs = np.zeros(\n",
        "            arrlist.shape[1]\n",
        "        )  # add all word2vec values into one vector for the sentence\n",
        "    for word in arrlist:\n",
        "      a_value = a / (a + get_word_frequency(word))  # smooth inverse frequency, SIF\n",
        "      vs = np.add(vs, np.multiply(a_value, word))  # vs += sif * word_vector\n",
        "    vs = np.divide(vs, arrlist.shape[0])  # weighted average\n",
        "    return vs"
      ],
      "metadata": {
        "id": "K23DWA1qJdar"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sem = []\n",
        "to = cx.get_shape()[0]\n",
        "# to = 2\n",
        "for i in range(to):\n",
        "  rx = cx.getrow(i).tocoo()\n",
        "\n",
        "  sorted_by_tfidf = sorted([(fv[j],v) for k,j,v in zip(rx.row, rx.col, rx.data)], key=lambda x: x[1], reverse=True)\n",
        "  # if i in [1,500,1000]:\n",
        "    # print(sorted_by_tfidf)\n",
        "\n",
        "  sorted_by_tfidf = list(filter(lambda x: x[0] in model_wv, sorted_by_tfidf))\n",
        "  arrlist = np.array( list(map(lambda x: model_wv[x[0]], sorted_by_tfidf[:5])  ))\n",
        "  if arrlist.shape[0]==0:\n",
        "    continue\n",
        "\n",
        "  # sem.append(np.mean(arrlist, axis=0))\n",
        "  sem.append(weighted(arrlist))\n",
        "\n",
        "sem = np.asarray(sem)\n",
        "# sem.shape,sem\n",
        "# np.save('/gdrive/MyDrive/minor_project_files/sentence_embeddings4.npy', sem)\n",
        "# np.save('/gdrive/MyDrive/minor_project_files/glove_sentence_embeddings4.npy', sem)\n",
        "# sem"
      ],
      "metadata": {
        "id": "Hlr9BemrwnRQ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# calculate PCA of this sentence set\n",
        "pca = PCA()\n",
        "pca.fit(np.array(sem))\n",
        "u = pca.components_[0]  # the PCA vector\n",
        "u = np.multiply(u, np.transpose(u))  # u x uT\n",
        "\n",
        "# pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
        "embedding_size=sem[0].shape[0]\n",
        "if len(u) < embedding_size:\n",
        "    for i in range(embedding_size - len(u)):\n",
        "        u = np.append(u, 0)  # add needed extension for multiplication below\n",
        "\n",
        "# resulting sentence vectors, vs = vs -u x uT x vs\n",
        "sentence_vecs = []\n",
        "for vs in sem:\n",
        "    sub = np.multiply(u, vs)\n",
        "    sentence_vecs.append(np.subtract(vs, sub))\n",
        "\n",
        "np.save('/gdrive/MyDrive/minor_project_files/arora_sentence_embeddings.npy', sentence_vecs)\n"
      ],
      "metadata": {
        "id": "1H1fl9egNAeM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf(corpus):\n",
        "    dic={}\n",
        "    for document in corpus:\n",
        "        for word in document:\n",
        "            if word in dic:\n",
        "                dic[word] = dic[word] + 1\n",
        "            else:\n",
        "                dic[word]=1\n",
        "    for word,freq in dic.items():\n",
        "        print(word,freq)\n",
        "        dic[word]=freq/sum(map(len, (document for document in corpus)))\n",
        "    return dic"
      ],
      "metadata": {
        "id": "KI07DivzEI_J"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sem = np.load('/gdrive/MyDrive/minor_project_files/arora_sentence_embeddings.npy', allow_pickle=True)\n",
        "print(sem[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWkKjayehxkE",
        "outputId": "b0a2cdda-e0b4-4ece-c043-295b95241494"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.88608253  0.78968546  0.03952108 -0.78146953  0.60086666  0.17499598\n",
            "   0.58080738 -0.06199354 -0.31637966  0.22862579  0.76320275 -0.449672\n",
            "  -0.45812576  0.54439217 -0.63675463  0.00681493  0.20754722 -0.54744711\n",
            "   0.31750661  0.23291712  0.48562898  0.32424532  0.29709069 -0.46910946\n",
            "   0.57720112  0.13064435 -0.55985368  0.25449161 -0.0810781   0.22339983\n",
            "  -0.33132478  1.2746254   0.17979356  0.71257594 -0.06345678  1.1007385\n",
            "   0.93899507 -0.04438215 -0.01715147 -0.04527296 -1.43924853  0.67309493\n",
            "  -0.15809807  0.23628887 -0.536597    0.16913407  0.1360056  -0.62645493\n",
            "   0.75002241  0.54410961 -0.24294168  0.51600161 -0.13280483 -0.43569982\n",
            "  -0.12644896  0.13436333 -0.5668966   0.21124034 -0.71325462 -0.0439922\n",
            "   0.73073836  0.73185005 -0.39510581 -0.43979219 -0.86810379  0.48561704\n",
            "   0.21382737  0.30997053  0.6713865   0.24731501 -0.25045833 -0.35981103\n",
            "  -0.44597959  0.41055525  0.67702342 -0.1420384  -0.00857112 -0.12346481\n",
            "   0.52801272  0.66797205  0.27854845  0.23178463  0.36424188 -0.01551782\n",
            "  -0.02586462  0.02087122  0.19705866  0.18026449 -0.10506731 -0.61647877\n",
            "  -0.29804147 -0.50627426  0.2399648  -0.03616626  0.24250822  0.15659745\n",
            "   0.12603044  0.02167382  0.28899856  1.08959422]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Annoy Index for finding approximate nearest neighbours (and corresponding label) "
      ],
      "metadata": {
        "id": "l1hhSnW6fbFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c5VbFcfcZM",
        "outputId": "60d328d7-4055-4ac5-a4de-12f95f36c164"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import annoy\n",
        "import pickle\n",
        "class AnnoyIndex():\n",
        "    def __init__(self,dimension):\n",
        "        self.dimension = dimension\n",
        "        self.index = annoy.AnnoyIndex(self.dimension)   \n",
        "   \n",
        "    def build(self,vectors, labels, number_of_trees=5):\n",
        "        self.vectors = vectors\n",
        "        self.labels = labels \n",
        "\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "          if not np.isnan(np.sum(vec)):\n",
        "            self.index.add_item(i, vec)\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(\n",
        "              list(vector), \n",
        "              k)                                           \n",
        "        return [self.labels[i] for i in indices]\n",
        "    def save(self,path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        print(label_path)\n",
        "        with open(label_path,'wb') as fp:\n",
        "            pickle.dump(self.labels,fp)\n",
        "        self.index.save(path)\n",
        "    \n",
        "    def load(self,path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        self.index=annoy.AnnoyIndex(self.dimension)\n",
        "        with open(label_path,\"rb\") as fp:\n",
        "            self.labels=pickle.load(fp)\n",
        "        self.index.load(path)"
      ],
      "metadata": {
        "id": "2WEjwnoBffNM"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "with open(dataset,\"r\") as fp:\n",
        "  questions=fp.readlines()"
      ],
      "metadata": {
        "id": "OfkGGxp3UGtP"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create annoy index from vectors\n",
        "index = AnnoyIndex(dimension=len(sem[0]))\n",
        "index.build(sem, questions)"
      ],
      "metadata": {
        "id": "hdujOv9ufneE"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index.save('/gdrive/MyDrive/minor_project_files/annoy_index.ann')\n",
        "# index.save('/gdrive/MyDrive/minor_project_files/annoy_index_glove.ann')\n",
        "index.save('/gdrive/MyDrive/minor_project_files/arora_sentence_embeddings.ann')"
      ],
      "metadata": {
        "id": "vEM0Zby5OtJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e3ff7b-10bb-4117-af5d-2a632e06dc0e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/minor_project_files/arora_sentence_embeddings.labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(sem[500])\n",
        "# print(questions[1])\n",
        "# print(sem[1])"
      ],
      "metadata": {
        "id": "GE085Mn0ft30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0853d6-32a4-4fe3-bad4-34fc72fd5c1e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the civil law examples?\\n',\n",
              " 'How can I increase my pay as a software engineer from mid 100K to over 200K a year, without working at a top tech company (Google, Facebook, and etc)?\\n',\n",
              " 'On which site can I buy the cheapest t-shirt?\\n',\n",
              " 'Which is the best smart phone between 8k to 13k?\\n',\n",
              " 'What is the hardest part of software deployment?\\n',\n",
              " 'Which graphic card is better NVidia Geforce GTX 950m (2 GB) or AMD Radeon m375 (4 gb)? Does the extra memory make that much difference in perfomance?\\n',\n",
              " 'What does the emoticon <3 mean?\\n',\n",
              " 'Have you read The Hunger Games?\\n',\n",
              " 'What are the most dangerous US cities? Why are they so dangerous?\\n',\n",
              " 'What are the effects of stress on the brain?\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  load existing annoy index from file\n",
        "loaded_index = AnnoyIndex(dimension=len(sem[0]))\n",
        "loaded_index.load('/gdrive/MyDrive/minor_project_files/arora_sentence_embeddings.ann')"
      ],
      "metadata": {
        "id": "xo2SUQlsOe5y"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index.query(sem[499])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWBGa3d0G_D7",
        "outputId": "cf28f5b4-5cd3-4ba6-a5a1-f3f8e6850b1b"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Which book is best for electronic devices?\\n',\n",
              " \"Russia supports China-Pakistan Economic Corridor (CPEC). How is this going to impact India's relations with Russia?\\n\",\n",
              " 'Why do police in the US use the Tonfa or ASP baton instead of nunchucks?\\n',\n",
              " 'Is Dunkirk movie based on a true story?\\n',\n",
              " 'What is the Lewis structure for KrF2? How is it determined?\\n',\n",
              " 'How good is a salary offer of 14k AED per month in Dubai compared to another offer of 1.5 lacs INR per month in Mumbai?\\n',\n",
              " 'Where is my Android app store revenue stored?\\n',\n",
              " 'What car engine oils can cause burning with white smoke when they are leaking?\\n',\n",
              " 'How can I retrieve a deleted message from my inbox on OkCupid?\\n',\n",
              " 'What are the best ways to get part time jobs in India?\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(input_question:str):\n",
        "    # get sentence embedding of the question\n",
        "    to_transform = word_tokenize(input_question)\n",
        "    matrix = vect.transform([to_transform])\n",
        "    # print(matrix)\n",
        "    cx = coo_matrix(matrix)\n",
        "\n",
        "    sorted_by_tfidf = sorted([(fv[j],v) for i,j,v in zip(cx.row, cx.col, cx.data)], key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # print(sorted_by_tfidf)\n",
        "    filtered = list(filter(lambda x: x[0] in model_wv, sorted_by_tfidf))\n",
        "\n",
        "    arrlist = np.array( list(map(lambda x: model_wv[x[0]], sorted_by_tfidf[:5])  ))\n",
        "\n",
        "    sentence_embedding = np.mean(arrlist, axis=0)\n",
        "    # print(sentence_embedding)\n",
        "    return index.query(sentence_embedding)"
      ],
      "metadata": {
        "id": "c53z2LkESBIh"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { run: \"auto\" }\n",
        "query = \"What are the best anime\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Finding relevant items in the index...\")\n",
        "print(get_similar(query))\n",
        "# %time query_embedding = get_similar(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oX_5ln1YB9W",
        "outputId": "e5d7e8a2-3b32-4c71-cba4-743ac3ed9b2c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding relevant items in the index...\n",
            "[\"Instagram reset my password but I don't know my email password to receive my new password. What do I do?\\n\", 'What are some good neutral colors for clothing?\\n', 'How can a person change a bad habit?\\n', 'What kind of music do you prefer?\\n', 'What should be preference order when you opt for service in a Civil Service examination?\\n', 'What foods should I try in India?\\n', 'Can an arranged marriage turn into a love marriage?\\n', 'Are we truly free?\\n', 'How do you search on Quora?\\n', 'Why do people use Quora when they could easily find the answer in a quick Google search?\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Flask on collab"
      ],
      "metadata": {
        "id": "CfEnbd3JTV-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!sudo apt update && sudo apt install ngrok\n",
        "!pip install flask_ngrok flask-bootstrap\n",
        "!pip install flask_restful flask_cors\n",
        "!cat /gdrive/MyDrive/minor_project_files/ngrok_token | xargs ngrok authtoken"
      ],
      "metadata": {
        "id": "moNKtS4sXFgR"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "stdout = sys.stdout\n",
        "stderr = sys.stderr"
      ],
      "metadata": {
        "id": "4L4DjSwU-Avm"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGs-FuZu_O7z",
        "outputId": "d9afb35e-2a12-42f1-df46-3d26a10874fb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template , request , jsonify\n",
        "from flask_restful import Resource, Api\n",
        "import os, logging, sys\n",
        "from flask_cors import CORS, cross_origin\n",
        "\n",
        "# sys.stdout = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"w\", buffering=1)\n",
        "# sys.stderr = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"a\", buffering=1)\n",
        "\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "# cors = CORS(app)\n",
        "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "api = Api(app)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "\n",
        "class Similarity(Resource):\n",
        "  # get endpoint to check server is up\n",
        "    def get(self):\n",
        "        return jsonify({\"hello\": \"Server Online!\"})\n",
        "\n",
        "    def post(self):\n",
        "        json_data = request.get_json(force=True)\n",
        "        qn = json_data[\"question\"]\n",
        "        # similarity = get_similar(qn)\n",
        "        # return list of questions\n",
        "        x = [\"question 1\", \"question 2\"]\n",
        "        return x\n",
        "\n",
        "\n",
        "api.add_resource(Similarity, \"/\")\n",
        "app.run()"
      ],
      "metadata": {
        "id": "Qp3oi91wTVVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.stdout = stdout\n",
        "sys.stderr = stderr"
      ],
      "metadata": {
        "id": "gRPs5eKO7nkt"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0Eps0Ou78w9"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}