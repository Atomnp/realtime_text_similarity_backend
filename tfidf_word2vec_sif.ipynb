{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_word2vec_sif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atomnp/realtime_text_similarity_backend/blob/main/tfidf_word2vec_sif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "CxExVdm3LZrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "HeRbR-2NAvAS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import coo_matrix, lil_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from typing import List\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wysvPmYtEjY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87661f07-5d74-47e3-eefe-fa67467c787d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting your google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xloBLoD9FfKw",
        "outputId": "c5981d67-046d-48af-fb5b-72031635eb21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the data"
      ],
      "metadata": {
        "id": "3KOwZpxvIimd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make shortcut of [this](https://drive.google.com/drive/folders/1BGr0cWKiJwT_jNg9nRNAhWgy0mYPgw_K?usp=sharing) folder in your gdrive**"
      ],
      "metadata": {
        "id": "cJnXuBlZG23A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = r'/gdrive/MyDrive/minor_project_files/filtered.txt'\n",
        "questions = pd.read_fwf(dataset, header=None, delimiter = \"\\n\", keep_default_na=False, na_values=['_'])"
      ],
      "metadata": {
        "id": "42wJ0pffSiRN"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoplist = list(string.punctuation) + list([\"``\", \"''\"])\n",
        "\n",
        "def preprocess(text):\n",
        "    stemmer = nltk.porter.PorterStemmer()\n",
        "    return [stemmer.stem(word.lower()) for word in word_tokenize(str(text)) if word.lower() not in stoplist and not word.isdigit()]"
      ],
      "metadata": {
        "id": "Q8IOXjAv3lBF"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed = questions[0].apply(preprocess).to_list()"
      ],
      "metadata": {
        "id": "sTHf4ut96D6G"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Model (Train, Save and Load)"
      ],
      "metadata": {
        "id": "mWALbqWcFBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "class callback(CallbackAny2Vec):\n",
        "    '''Callback to print loss after each epoch.'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "      loss = model.get_latest_training_loss()\n",
        "      if self.epoch == 0:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "      else:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "      self.epoch += 1\n",
        "      self.loss_previous_step = loss"
      ],
      "metadata": {
        "id": "2sNEI7X9M3di"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if you want to retrain the word2vec model\n",
        "# it_copy, sentences = itertools.tee(sentences)\n",
        "# model = Word2Vec(sentences=processed, size=100, window=5, min_count=1, workers=4, compute_loss=True, iter=50, callbacks=[callback()])\n",
        "# model.save(\"/gdrive/MyDrive/minor_project_files/word2vec_50_iter.model\")"
      ],
      "metadata": {
        "id": "GDD83pHX8FaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4398b6e-372c-4ef5-8b19-61d368bd73e2"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 1622462.125\n",
            "Loss after epoch 1: 1312306.625\n",
            "Loss after epoch 2: 1184831.0\n",
            "Loss after epoch 3: 1031265.75\n",
            "Loss after epoch 4: 992155.0\n",
            "Loss after epoch 5: 1004180.5\n",
            "Loss after epoch 6: 1014405.0\n",
            "Loss after epoch 7: 857419.0\n",
            "Loss after epoch 8: 793878.0\n",
            "Loss after epoch 9: 818756.0\n",
            "Loss after epoch 10: 784788.0\n",
            "Loss after epoch 11: 814961.0\n",
            "Loss after epoch 12: 820393.0\n",
            "Loss after epoch 13: 781218.0\n",
            "Loss after epoch 14: 797458.0\n",
            "Loss after epoch 15: 754276.0\n",
            "Loss after epoch 16: 799526.0\n",
            "Loss after epoch 17: 768215.0\n",
            "Loss after epoch 18: 641368.0\n",
            "Loss after epoch 19: 647118.0\n",
            "Loss after epoch 20: 635604.0\n",
            "Loss after epoch 21: 624394.0\n",
            "Loss after epoch 22: 615802.0\n",
            "Loss after epoch 23: 662708.0\n",
            "Loss after epoch 24: 625012.0\n",
            "Loss after epoch 25: 665296.0\n",
            "Loss after epoch 26: 618182.0\n",
            "Loss after epoch 27: 614952.0\n",
            "Loss after epoch 28: 597666.0\n",
            "Loss after epoch 29: 609306.0\n",
            "Loss after epoch 30: 615836.0\n",
            "Loss after epoch 31: 605878.0\n",
            "Loss after epoch 32: 618628.0\n",
            "Loss after epoch 33: 590812.0\n",
            "Loss after epoch 34: 590256.0\n",
            "Loss after epoch 35: 620624.0\n",
            "Loss after epoch 36: 618512.0\n",
            "Loss after epoch 37: 599648.0\n",
            "Loss after epoch 38: 588462.0\n",
            "Loss after epoch 39: 593910.0\n",
            "Loss after epoch 40: 548644.0\n",
            "Loss after epoch 41: 559170.0\n",
            "Loss after epoch 42: 568982.0\n",
            "Loss after epoch 43: 543448.0\n",
            "Loss after epoch 44: 569440.0\n",
            "Loss after epoch 45: 397896.0\n",
            "Loss after epoch 46: 312700.0\n",
            "Loss after epoch 47: 306880.0\n",
            "Loss after epoch 48: 306116.0\n",
            "Loss after epoch 49: 302072.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load already saved word2vec model\n",
        "model = Word2Vec.load(\"/gdrive/MyDrive/minor_project_files/word2vec_50_iter.model\")"
      ],
      "metadata": {
        "id": "6N1UnBrUORUe"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('china')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgkzTCEtGQRn",
        "outputId": "62fdfb1d-7a77-443d-84fe-c3be53929df7"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('taiwan', 0.7596267461776733),\n",
              " ('pakistan', 0.7264149188995361),\n",
              " ('japan', 0.7235797643661499),\n",
              " ('russia', 0.7100479602813721),\n",
              " ('afghanistan', 0.677221417427063),\n",
              " ('malaysia', 0.6441991329193115),\n",
              " ('iran', 0.6398093104362488),\n",
              " ('franc', 0.6356956958770752),\n",
              " ('israel', 0.6324349641799927),\n",
              " ('mexico', 0.6236512064933777)]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find sentence Embeddings for each sentence in the dataset\n",
        "\n",
        "1.   Lookup their word vectors from word2vec/glove model\n",
        "3.   Save the weighted average word vector as the sentence embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "ehay95iFH0o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: get a proper word frequency for a word in a document set\n",
        "# or perhaps just a typical frequency for a word from Google's n-grams\n",
        "def get_word_frequency(word_text):\n",
        "    return 0.0001  # set to a low occurring frequency - probably not unrealistic for most words, improves vector values"
      ],
      "metadata": {
        "id": "Td3Aqf6KJA_g"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_average(sentence: List[str], embedding_size=100, a: float = 1e-3):\n",
        "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "    for word in sentence:\n",
        "      a_value = a / (a + get_word_frequency(word))  # smooth inverse frequency, SIF\n",
        "      vs = np.add(vs, np.multiply(a_value, model.wv[word]))  # vs += sif * word_vector\n",
        "    vs = np.divide(vs, len(sentence))  # weighted average\n",
        "    return vs"
      ],
      "metadata": {
        "id": "ERvaXAzOZBk7"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "\n",
        "def sentences_to_vec(sentences: List[List[str]], embedding_size=100, a=1e-3):\n",
        "    global pca\n",
        "    sentence_set = [weighted_average(sentence) for sentence in sentences]\n",
        "\n",
        "    # calculate PCA of this sentence set\n",
        "    pca.fit(np.array(sentence_set))\n",
        "    u = pca.components_[0]  # the PCA vector\n",
        "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
        "\n",
        "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
        "    if len(u) < embedding_size:\n",
        "        for i in range(embedding_size - len(u)):\n",
        "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
        "\n",
        "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
        "    sentence_vecs = []\n",
        "    for vs in sentence_set:\n",
        "        sub = np.multiply(u, vs)\n",
        "        sentence_vecs.append(np.subtract(vs, sub))\n",
        "    \n",
        "    return sentence_vecs"
      ],
      "metadata": {
        "id": "_ZzuQu8tLQjn"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences_vecs = np.asarray(sentences_to_vec(processed))\n",
        "# np.save('/gdrive/MyDrive/minor_project_files/weighted_sentence_embeddings.npy', sentence_vecs)"
      ],
      "metadata": {
        "id": "o4hyGzK5HrDh"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_vecs = np.load('/gdrive/MyDrive/minor_project_files/weighted_sentence_embeddings.npy', allow_pickle=True)\n",
        "print(sentences_vecs[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWkKjayehxkE",
        "outputId": "9f81f1fe-803f-4e91-e09e-c8cf682c0aa0"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-9.80927748e-01 -2.79434575e-01 -5.66141313e-03 -1.61609953e-01\n",
            "  -3.06725165e-01 -2.36819868e-01  3.75395380e-01  4.79206396e-01\n",
            "   1.33139029e-01 -6.97573714e-01 -5.39033370e-01  2.65995558e-01\n",
            "   4.17935940e-01 -2.03088195e-01  3.17525126e-03  1.81194688e-01\n",
            "  -1.50001279e-01  7.83060354e-01  6.14964515e-01  1.05875483e-01\n",
            "  -5.68448379e-02  6.09423031e-01  3.57881941e-01 -4.77975181e-01\n",
            "  -9.36058938e-01  7.34957507e-01  4.21115536e-01  8.08080726e-02\n",
            "  -6.37149807e-01  1.01189915e-01  1.36324577e-02  1.04379010e-01\n",
            "  -5.95440226e-01  4.24879820e-02 -2.31063301e-02 -2.51275549e-01\n",
            "  -2.36971548e-02 -6.24342715e-02 -3.17679840e-01 -8.77911832e-01\n",
            "  -5.27627447e-01  4.40916404e-01  1.68490753e-01 -4.38635626e-02\n",
            "  -2.33046105e-01 -8.95342083e-02  4.68331654e-01  3.28619166e-01\n",
            "   5.35630283e-01  2.01373408e-01 -3.95593916e-01  1.88952985e-01\n",
            "  -8.51291801e-04 -7.98133247e-02  1.80502431e-01 -5.99805941e-01\n",
            "  -4.49169215e-02 -2.41230510e-01 -3.01091673e-01 -2.07823346e-01\n",
            "  -8.00059618e-01  5.05780786e-01  5.80667000e-01 -4.30758742e-01\n",
            "   2.88914952e-01  5.48676379e-01  1.76689866e-01  1.87263280e-01\n",
            "  -8.17392702e-01 -1.81792544e-01 -1.94119883e-01 -5.48347378e-01\n",
            "  -1.49246112e-01 -4.57568587e-01 -5.56064672e-01  4.19274237e-01\n",
            "  -4.06524635e-01  4.29458392e-01 -3.92627006e-01 -2.91394611e-01\n",
            "  -1.19016405e+00  1.94614155e-01  1.62631419e-01  2.37525533e-03\n",
            "  -8.97229647e-02  2.63033512e-01  5.26513986e-01  4.18872779e-01\n",
            "  -4.19092237e-01  9.90520872e-02 -7.10118909e-01  8.15479063e-02\n",
            "   7.35988634e-01 -2.77728947e-01  3.22989314e-01  1.57827054e-01\n",
            "   2.29958551e-01 -4.74360275e-01  6.82652483e-01 -5.77630357e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Annoy Index for finding approximate nearest neighbours (and corresponding label) "
      ],
      "metadata": {
        "id": "l1hhSnW6fbFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c5VbFcfcZM",
        "outputId": "683d04b5-a2ed-46f3-992f-eea39781ab30"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import annoy\n",
        "import pickle\n",
        "\n",
        "class AnnoyIndex():\n",
        "    def __init__(self, dimension):\n",
        "        self.dimension = dimension\n",
        "        self.index = annoy.AnnoyIndex(self.dimension)   \n",
        "   \n",
        "    def build(self, vectors, labels, number_of_trees=5):\n",
        "        self.vectors = vectors\n",
        "        self.labels = labels \n",
        "\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "          if not np.isnan(np.sum(vec)):\n",
        "            self.index.add_item(i, vec)\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(list(vector), k)                                           \n",
        "        return [self.labels[i] for i in indices]\n",
        "    \n",
        "    def save(self, path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        print(label_path)\n",
        "        with open(label_path,'wb') as fp:\n",
        "            pickle.dump(self.labels,fp)\n",
        "        self.index.save(path)\n",
        "    \n",
        "    def load(self, path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        self.index=annoy.AnnoyIndex(self.dimension)\n",
        "        with open(label_path,\"rb\") as fp:\n",
        "            self.labels=pickle.load(fp)\n",
        "        self.index.load(path)"
      ],
      "metadata": {
        "id": "2WEjwnoBffNM"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "with open(dataset, \"r\") as fp:\n",
        "  questions=[line.strip() for line in fp.readlines()]"
      ],
      "metadata": {
        "id": "OfkGGxp3UGtP"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create annoy index from vectors\n",
        "index = AnnoyIndex(dimension=len(sentences_vecs[0]))\n",
        "index.build(sentences_vecs, questions)"
      ],
      "metadata": {
        "id": "hdujOv9ufneE"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.save('/gdrive/MyDrive/minor_project_files/weighted_annoy_index.ann')"
      ],
      "metadata": {
        "id": "vEM0Zby5OtJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c24bab-d0f0-4312-8834-bc45cd5fe9a8"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/minor_project_files/weighted_annoy_index.labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(sentences_vecs[500])"
      ],
      "metadata": {
        "id": "GE085Mn0ft30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f167c296-ed87-422a-c35e-ca2d9a35fdcb"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the civil law examples?',\n",
              " \"Should I take up a PhD in applied math if the advisor i'm going to work with does not have any awards listed in his CV?\",\n",
              " 'What are the Indian deities?',\n",
              " 'How is US education different from Indian education?',\n",
              " 'What are the major aspects of the Victorian literature?',\n",
              " 'What are the Class Definitions ?',\n",
              " 'What are the sociological perspectives?',\n",
              " 'What are the top 10 books for Christian apologetics?',\n",
              " 'What is the penalty for espionage?',\n",
              " 'What is the property tax rate in Granville, Ohio? How is it compared to the one of Wyoming?']"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Annoy Index and query sentences"
      ],
      "metadata": {
        "id": "OPCC5LCSRUzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  load existing annoy index from file\n",
        "loaded_index = AnnoyIndex(dimension=len(sentences_vecs[0]))\n",
        "loaded_index.load('/gdrive/MyDrive/minor_project_files/weighted_annoy_index.ann')"
      ],
      "metadata": {
        "id": "xo2SUQlsOe5y"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index.query(sentences_vecs[500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWBGa3d0G_D7",
        "outputId": "d5a719a1-5981-487f-8fb6-1f1b436aa27b"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the civil law examples?',\n",
              " \"Should I take up a PhD in applied math if the advisor i'm going to work with does not have any awards listed in his CV?\",\n",
              " 'What are the Indian deities?',\n",
              " 'How is US education different from Indian education?',\n",
              " 'What are the major aspects of the Victorian literature?',\n",
              " 'What are the Class Definitions ?',\n",
              " 'What are the sociological perspectives?',\n",
              " 'What are the top 10 books for Christian apologetics?',\n",
              " 'What is the penalty for espionage?',\n",
              " 'What is the property tax rate in Granville, Ohio? How is it compared to the one of Wyoming?']"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_vec(sentence: str, embedding_size=100, a=1e-3):\n",
        "    vs = np.zeros(embedding_size)\n",
        "    for word in sentence:\n",
        "      a_value = a / (a + get_word_frequency(word))\n",
        "      vs = np.add(vs, np.multiply(a_value, model.wv[word]))\n",
        "    vs = np.divide(vs, len(sentence))\n",
        "\n",
        "    # calculate PCA of this sentence\n",
        "    pca.transform([vs])\n",
        "    u = pca.components_[0]  # the PCA vector\n",
        "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
        "\n",
        "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
        "    if len(u) < embedding_size:\n",
        "        for i in range(embedding_size - len(u)):\n",
        "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
        "\n",
        "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
        "    return np.subtract(vs, np.multiply(u, vs))"
      ],
      "metadata": {
        "id": "RcchdkULWPiR"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(input_question: str):\n",
        "    # get sentence embedding of the question\n",
        "    to_transform = word_tokenize(input_question.lower())\n",
        "    embedding = sentence_to_vec(to_transform)\n",
        "    return index.query(embedding)"
      ],
      "metadata": {
        "id": "c53z2LkESBIh"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { run: \"auto\" }\n",
        "query = \"What are the best book\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Finding relevant items in the index...\\n\")\n",
        "for similar in get_similar(query):\n",
        "    print(similar)\n",
        "print()\n",
        "%time query_embedding = get_similar(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oX_5ln1YB9W",
        "outputId": "6a8be9cf-94a6-44fb-da37-70ffc8f63176"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding relevant items in the index...\n",
            "\n",
            "Could Julian Albert, played by Tom Felton, be Doctor Alchemy on Flash?\n",
            "What are the best YA books?\n",
            "What are the best life-changing books?\n",
            "What are the best Bengali books?\n",
            "What is your review of Shahrukh Khan (actor)?\n",
            "What are the best Non-fiction books currently?\n",
            "What are the best calculus books?\n",
            "Is it possible to download movies from Netflix?\n",
            "Is it possible to slip on a banana peel?\n",
            "What are the best books in neuroscience?\n",
            "\n",
            "CPU times: user 1.32 ms, sys: 5 ms, total: 6.31 ms\n",
            "Wall time: 6.61 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Flask on collab"
      ],
      "metadata": {
        "id": "CfEnbd3JTV-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!sudo apt update && sudo apt install ngrok\n",
        "!pip install flask_ngrok flask-bootstrap\n",
        "!pip install flask_restful flask_cors\n",
        "!cat /gdrive/MyDrive/minor_project_files/ngrok_token | xargs ngrok authtoken"
      ],
      "metadata": {
        "id": "moNKtS4sXFgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "stdout = sys.stdout\n",
        "stderr = sys.stderr"
      ],
      "metadata": {
        "id": "4L4DjSwU-Avm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGs-FuZu_O7z",
        "outputId": "d9afb35e-2a12-42f1-df46-3d26a10874fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template , request , jsonify\n",
        "from flask_restful import Resource, Api\n",
        "import os, logging, sys\n",
        "from flask_cors import CORS, cross_origin\n",
        "\n",
        "# sys.stdout = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"w\", buffering=1)\n",
        "# sys.stderr = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"a\", buffering=1)\n",
        "\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "# cors = CORS(app)\n",
        "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "api = Api(app)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "\n",
        "class Similarity(Resource):\n",
        "  # get endpoint to check server is up\n",
        "    def get(self):\n",
        "        return jsonify({\"hello\": \"Server Online!\"})\n",
        "\n",
        "    def post(self):\n",
        "        json_data = request.get_json(force=True)\n",
        "        qn = json_data[\"question\"]\n",
        "        # similarity = get_similar(qn)\n",
        "        # return list of questions\n",
        "        x = [\"question 1\", \"question 2\"]\n",
        "        return x\n",
        "\n",
        "\n",
        "api.add_resource(Similarity, \"/\")\n",
        "app.run()"
      ],
      "metadata": {
        "id": "Qp3oi91wTVVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.stdout = stdout\n",
        "sys.stderr = stderr"
      ],
      "metadata": {
        "id": "gRPs5eKO7nkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0Eps0Ou78w9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}