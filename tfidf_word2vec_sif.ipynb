{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_word2vec_sif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atomnp/realtime_text_similarity_backend/blob/main/tfidf_word2vec_sif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "CxExVdm3LZrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HeRbR-2NAvAS"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.sparse import coo_matrix, lil_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from typing import List\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wysvPmYtEjY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57323c83-3891-4ab4-eed1-3ab02f244b07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting your google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xloBLoD9FfKw",
        "outputId": "7df519be-7f7c-4cf7-b178-81a296e7abcc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the data"
      ],
      "metadata": {
        "id": "3KOwZpxvIimd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make shortcut of [this](https://drive.google.com/drive/folders/1BGr0cWKiJwT_jNg9nRNAhWgy0mYPgw_K?usp=sharing) folder in your gdrive**"
      ],
      "metadata": {
        "id": "cJnXuBlZG23A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = r'/gdrive/MyDrive/minor_project_files/filtered.txt'\n",
        "questions = pd.read_fwf(dataset, header=None, delimiter = \"\\n\", keep_default_na=False, na_values=['_'])"
      ],
      "metadata": {
        "id": "42wJ0pffSiRN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoplist = nltk.corpus.stopwords.words('english') + list(string.punctuation) + list([\"``\", \"''\"])\n",
        "\n",
        "def preprocess(text):\n",
        "    # stemmer = nltk.porter.PorterStemmer()\n",
        "    return [word.lower() for word in word_tokenize(str(text)) if word.lower() not in stoplist and not word.isdigit()]"
      ],
      "metadata": {
        "id": "Q8IOXjAv3lBF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed = questions[0].apply(preprocess).to_list()"
      ],
      "metadata": {
        "id": "sTHf4ut96D6G"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tm_SByTie6B",
        "outputId": "163e0166-54b5-4e99-b7ed-0345fc3f4e36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['space',\n",
              "  'inserted',\n",
              "  'first',\n",
              "  'letter',\n",
              "  'words',\n",
              "  'text',\n",
              "  'ms',\n",
              "  'word',\n",
              "  'eg',\n",
              "  'q',\n",
              "  'uora',\n",
              "  'instead',\n",
              "  'quora',\n",
              "  'fix'],\n",
              " [\"'s\", 'like', 'work', 'care.com', 'first', 'job'],\n",
              " ['german', 'jews', 'treated', 'hitler', 'wwi'],\n",
              " ['sugar', 'bad', 'us'],\n",
              " ['deal', 'death', 'grandparent'],\n",
              " [\"'s\",\n",
              "  'best',\n",
              "  'available',\n",
              "  'compact',\n",
              "  'camera',\n",
              "  'sony',\n",
              "  'cyber-shot',\n",
              "  'dsc-rx100',\n",
              "  'anything',\n",
              "  'better',\n",
              "  'coming',\n",
              "  'next',\n",
              "  '2-3',\n",
              "  'months'],\n",
              " ['best', 'way', 'build', 'email', 'list'],\n",
              " ['humanity', 'part', 'experiment', 'someone', \"'s\", 'terrarium'],\n",
              " ['unix', 'downloadable'],\n",
              " ['calculate', 'chemical', 'formula', 'ammonium', 'chlorate']]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Model (Train, Save and Load)"
      ],
      "metadata": {
        "id": "mWALbqWcFBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "class callback(CallbackAny2Vec):\n",
        "    '''Callback to print loss after each epoch.'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "      loss = model.get_latest_training_loss()\n",
        "      if self.epoch == 0:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "      else:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "      self.epoch += 1\n",
        "      self.loss_previous_step = loss"
      ],
      "metadata": {
        "id": "2sNEI7X9M3di"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if you want to retrain the word2vec model\n",
        "# it_copy, sentences = itertools.tee(sentences)\n",
        "model = Word2Vec(sentences=processed, size=100, window=5, min_count=1, workers=4, compute_loss=True, iter=5, callbacks=[callback()])\n",
        "model.save(\"/gdrive/MyDrive/minor_project_files/word2vec_6_iter.model\")"
      ],
      "metadata": {
        "id": "GDD83pHX8FaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784378cc-963e-4633-d884-5ad02d52ac4e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 1376979.625\n",
            "Loss after epoch 1: 948915.625\n",
            "Loss after epoch 2: 806411.25\n",
            "Loss after epoch 3: 789229.25\n",
            "Loss after epoch 4: 672099.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load already saved word2vec model\n",
        "model = Word2Vec.load(\"/gdrive/MyDrive/minor_project_files/word2vec_6_iter.model\")"
      ],
      "metadata": {
        "id": "6N1UnBrUORUe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('movie')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgkzTCEtGQRn",
        "outputId": "eb9f6098-15c4-4237-e5e1-cb29c913f33d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('actor/actress', 0.8493589162826538),\n",
              " ('bollywood', 0.8430700302124023),\n",
              " ('film', 0.8417855501174927),\n",
              " ('actor', 0.8351795673370361),\n",
              " ('scenes', 0.8346238136291504),\n",
              " ('movies', 0.834228515625),\n",
              " ('scene', 0.8281405568122864),\n",
              " ('actress', 0.8274914622306824),\n",
              " ('films', 0.8154959678649902),\n",
              " ('hollywood', 0.8133722543716431)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find sentence Embeddings for each sentence in the dataset\n",
        "\n",
        "1.   Lookup their word vectors from word2vec/glove model\n",
        "3.   Save the weighted average word vector as the sentence embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "ehay95iFH0o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "wf = Counter([word for sentence in processed for word in sentence])"
      ],
      "metadata": {
        "id": "Dq4DqiIyx0l-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = sum(wf.values())\n",
        "print(unique_words, wf[\"movie\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7TMqAb0x2Xx",
        "outputId": "ea4e0851-8e1a-4db6-eff5-700897208c94"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1698817 2073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# todo: get a proper word frequency for a word in a document set\n",
        "# or perhaps just a typical frequency for a word from Google's n-grams\n",
        "def get_word_frequency(word_text):\n",
        "    return wf[word_text]/unique_words  "
      ],
      "metadata": {
        "id": "Td3Aqf6KJA_g"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_average(sentence: List[str], embedding_size=100, a: float = 1e-3):\n",
        "    vs = np.zeros(embedding_size)  # add all word2vec values into one vector for the sentence\n",
        "    for word in sentence:\n",
        "      a_value = a / (a + get_word_frequency(word))  # smooth inverse frequency, SIF\n",
        "      vs = np.add(vs, np.multiply(a_value, model.wv[word]))  # vs += sif * word_vector\n",
        "    \n",
        "    vs = np.divide(vs, 1 if not len(sentence) else len(sentence))  # weighted average\n",
        "    return vs"
      ],
      "metadata": {
        "id": "ERvaXAzOZBk7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "\n",
        "def sentences_to_vec(sentences: List[List[str]], embedding_size=100, a=1e-3):\n",
        "    global pca\n",
        "    sentence_set = [weighted_average(sentence) for sentence in sentences]\n",
        "    \n",
        "    # calculate PCA of this sentence set\n",
        "    pca.fit(np.array(sentence_set))\n",
        "    u = pca.components_[0]  # the PCA vector\n",
        "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
        "\n",
        "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
        "    if len(u) < embedding_size:\n",
        "        for i in range(embedding_size - len(u)):\n",
        "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
        "\n",
        "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
        "    sentence_vecs = []\n",
        "    for vs in sentence_set:\n",
        "        sub = np.multiply(u, vs)\n",
        "        sentence_vecs.append(np.subtract(vs, sub))\n",
        "    \n",
        "    return sentence_vecs"
      ],
      "metadata": {
        "id": "_ZzuQu8tLQjn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_vecs = np.asarray(sentences_to_vec(processed))\n",
        "np.save('/gdrive/MyDrive/minor_project_files/weighted_sentence_embeddings.npy', sentences_vecs)"
      ],
      "metadata": {
        "id": "o4hyGzK5HrDh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_to_vec(processed[:10])"
      ],
      "metadata": {
        "id": "uNvYCD1-mC22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_vecs = np.load('/gdrive/MyDrive/minor_project_files/weighted_sentence_embeddings.npy', allow_pickle=True)\n",
        "print(sentences_vecs[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWkKjayehxkE",
        "outputId": "de9825fb-8af8-46e3-cb1e-3f9f1c878d9f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.04123472 -0.62800549  0.28561507 -0.33221452  0.02482206 -0.35611167\n",
            "  -0.46764914  0.20503985  0.0881743   0.14388739 -0.24019077 -0.27652836\n",
            "  -0.05590096  0.46147946 -0.1993378   0.18196125  0.02341565 -0.27972645\n",
            "  -0.23335134  0.34333249 -0.16804745  0.58046789  0.32486065 -0.21757168\n",
            "   0.09604735 -0.37448978  0.23501296 -0.04423243  0.1940462  -0.31905368\n",
            "  -0.28384971  0.11100448 -0.08584574 -0.28831921  0.02981178  0.51389128\n",
            "   0.49132991 -0.33462451 -0.18148645  0.28262717  0.04049196  0.37171954\n",
            "  -0.05181293  0.40783267 -0.17894445 -0.3326019  -0.18167464 -0.01213471\n",
            "   0.05711866 -0.1105276  -0.24635505 -0.08920633  0.20526904  0.34831443\n",
            "   0.38948737  0.130034   -0.03987007  0.01378547  0.18242859  0.10977452\n",
            "  -0.14724109 -0.05026363  0.09099263  0.11086938  0.23176335  0.29164669\n",
            "   0.1466073  -0.0049128  -0.00975062 -0.53459251  0.17689424  0.27955076\n",
            "   0.19985334  0.477654    0.12803252  0.01984706  0.72792506  0.06607801\n",
            "  -0.14808793 -0.17412628 -0.17884588  0.28337336 -0.31325177 -0.33208535\n",
            "   0.0110938  -0.01547725  0.0483112  -0.16432963  0.25463563  0.07533448\n",
            "   0.02265629  0.21495529 -0.12342364  0.22632525  0.17052011 -0.10653874\n",
            "  -0.24648692 -0.3727349  -0.33993415 -0.79009053]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Annoy Index for finding approximate nearest neighbours (and corresponding label) "
      ],
      "metadata": {
        "id": "l1hhSnW6fbFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c5VbFcfcZM",
        "outputId": "65a22d4f-472b-42e8-a407-21024ee19bc3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 4.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391666 sha256=59228d97e26895bda30b12debd6dc2435c32a767dbc28236d563d500d2cf8d87\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import annoy\n",
        "import pickle\n",
        "\n",
        "class AnnoyIndex():\n",
        "    def __init__(self, dimension):\n",
        "        self.dimension = dimension\n",
        "        self.index = annoy.AnnoyIndex(self.dimension)   \n",
        "   \n",
        "    def build(self, vectors, labels, number_of_trees=5):\n",
        "        self.vectors = vectors\n",
        "        self.labels = labels \n",
        "\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "          if not np.isnan(np.sum(vec)):\n",
        "            self.index.add_item(i, vec)\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(list(vector), k)                                           \n",
        "        return [self.labels[i] for i in indices]\n",
        "    \n",
        "    def save(self, path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        print(label_path)\n",
        "        with open(label_path,'wb') as fp:\n",
        "            pickle.dump(self.labels,fp)\n",
        "        self.index.save(path)\n",
        "    \n",
        "    def load(self, path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        self.index=annoy.AnnoyIndex(self.dimension)\n",
        "        with open(label_path,\"rb\") as fp:\n",
        "            self.labels=pickle.load(fp)\n",
        "        self.index.load(path)"
      ],
      "metadata": {
        "id": "2WEjwnoBffNM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "with open(dataset, \"r\") as fp:\n",
        "  questions=[line.strip() for line in fp.readlines()]"
      ],
      "metadata": {
        "id": "OfkGGxp3UGtP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create annoy index from vectors\n",
        "index = AnnoyIndex(dimension=len(sentences_vecs[0]))\n",
        "index.build(sentences_vecs, questions)"
      ],
      "metadata": {
        "id": "hdujOv9ufneE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.save('/gdrive/MyDrive/minor_project_files/weighted_annoy_index.ann')"
      ],
      "metadata": {
        "id": "vEM0Zby5OtJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ccc749-fa50-442e-fd01-d990e6299250"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/minor_project_files/weighted_annoy_index.labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(sentences_vecs[500])"
      ],
      "metadata": {
        "id": "GE085Mn0ft30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b348bcb4-4fa8-4376-c030-efb4a1a52d1b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the civil law examples?',\n",
              " 'What is a outstanding certification? What purpose does it serve?',\n",
              " 'What is the difference between a criminal law and a civil law? Is murder a criminal offence or a civil offence?',\n",
              " 'What are some examples of the Third Law of Thermodynamics?',\n",
              " 'What is an example of the law of interaction?',\n",
              " 'What would Stannis have done with Sansa if he had won the Battle of Blackwater?',\n",
              " 'What is the parallelogram law of forces? What are some examples in how it is used?',\n",
              " 'What are the best books on employment and labor law?',\n",
              " 'Why do babies cry soon after they are delivered? What is the scientific reason behind it?',\n",
              " 'Do babies also feel the pain of child birth?']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Annoy Index and query sentences"
      ],
      "metadata": {
        "id": "OPCC5LCSRUzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  load existing annoy index from file\n",
        "loaded_index = AnnoyIndex(dimension=len(sentences_vecs[0]))\n",
        "loaded_index.load('/gdrive/MyDrive/minor_project_files/weighted_annoy_index.ann')"
      ],
      "metadata": {
        "id": "xo2SUQlsOe5y"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index.query(sentences_vecs[500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWBGa3d0G_D7",
        "outputId": "bab9648c-300f-4847-e7a9-0bb9b48ca91a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the civil law examples?',\n",
              " 'What is a outstanding certification? What purpose does it serve?',\n",
              " 'What is the difference between a criminal law and a civil law? Is murder a criminal offence or a civil offence?',\n",
              " 'What are some examples of the Third Law of Thermodynamics?',\n",
              " 'What is an example of the law of interaction?',\n",
              " 'What would Stannis have done with Sansa if he had won the Battle of Blackwater?',\n",
              " 'What is the parallelogram law of forces? What are some examples in how it is used?',\n",
              " 'What are the best books on employment and labor law?',\n",
              " 'Why do babies cry soon after they are delivered? What is the scientific reason behind it?',\n",
              " 'Do babies also feel the pain of child birth?']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_vec(sentence: str, embedding_size=100, a=1e-3):\n",
        "    vs = np.zeros(embedding_size)\n",
        "    for word in sentence:\n",
        "      a_value = a / (a + get_word_frequency(word))\n",
        "      if word in model.wv:\n",
        "          vs = np.add(vs, np.multiply(a_value, model.wv[word]))\n",
        "    vs = np.divide(vs, 1 if not len(sentence) else len(sentence))\n",
        "\n",
        "    # calculate PCA of this sentence\n",
        "    # pca.transform([vs])\n",
        "    u = pca.components_[0]  # the PCA vector\n",
        "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
        "\n",
        "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
        "    if len(u) < embedding_size:\n",
        "        for i in range(embedding_size - len(u)):\n",
        "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
        "\n",
        "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
        "    return np.subtract(vs, np.multiply(u, vs))"
      ],
      "metadata": {
        "id": "RcchdkULWPiR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(input_question: str):\n",
        "    # get sentence embedding of the question\n",
        "    to_transform = word_tokenize(input_question.lower())\n",
        "    embedding = sentence_to_vec(to_transform)\n",
        "    return index.query(embedding)"
      ],
      "metadata": {
        "id": "c53z2LkESBIh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { run: \"auto\" }\n",
        "query = \"should i buy the new macbook\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Finding relevant items in the index...\\n\")\n",
        "for similar in get_similar(query):\n",
        "    print(similar)\n",
        "print()\n",
        "%time query_embedding = get_similar(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oX_5ln1YB9W",
        "outputId": "3e64974d-bfa6-4ad7-f7c7-80dc5e7b3fb8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding relevant items in the index...\n",
            "\n",
            "Is it better to buy a new or a used Porsche?\n",
            "How can we solve racism?\n",
            "How do I root the XOLO Q1010i?\n",
            "Should I buy Battlefield 4 at Best Buy?\n",
            "Why do people put ridiculous questions on Quora when they can just Google them? Huh, Huh, Huh :-/\n",
            "What's the best way to meet new people in LA?\n",
            "What is the difference between 4 wheel drive, 2 wheel drive and all wheel drive?\n",
            "How do you make a milkshake?\n",
            "What is your opinion about Narendra Modi's speech in the joint session of USA Congress?\n",
            "Which are some professional photography jobs?\n",
            "\n",
            "CPU times: user 513 µs, sys: 21 µs, total: 534 µs\n",
            "Wall time: 544 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Flask on collab"
      ],
      "metadata": {
        "id": "CfEnbd3JTV-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!sudo apt update && sudo apt install ngrok\n",
        "!pip install flask_ngrok flask-bootstrap\n",
        "!pip install flask_restful flask_cors\n",
        "!cat /gdrive/MyDrive/minor_project_files/ngrok_token | xargs ngrok authtoken"
      ],
      "metadata": {
        "id": "moNKtS4sXFgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "stdout = sys.stdout\n",
        "stderr = sys.stderr"
      ],
      "metadata": {
        "id": "4L4DjSwU-Avm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGs-FuZu_O7z",
        "outputId": "d9afb35e-2a12-42f1-df46-3d26a10874fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template , request , jsonify\n",
        "from flask_restful import Resource, Api\n",
        "import os, logging, sys\n",
        "from flask_cors import CORS, cross_origin\n",
        "\n",
        "# sys.stdout = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"w\", buffering=1)\n",
        "# sys.stderr = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"a\", buffering=1)\n",
        "\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "# cors = CORS(app)\n",
        "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "api = Api(app)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "\n",
        "class Similarity(Resource):\n",
        "  # get endpoint to check server is up\n",
        "    def get(self):\n",
        "        return jsonify({\"hello\": \"Server Online!\"})\n",
        "\n",
        "    def post(self):\n",
        "        json_data = request.get_json(force=True)\n",
        "        qn = json_data[\"question\"]\n",
        "        # similarity = get_similar(qn)\n",
        "        # return list of questions\n",
        "        x = [\"question 1\", \"question 2\"]\n",
        "        return x\n",
        "\n",
        "\n",
        "api.add_resource(Similarity, \"/\")\n",
        "app.run()"
      ],
      "metadata": {
        "id": "Qp3oi91wTVVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.stdout = stdout\n",
        "sys.stderr = stderr"
      ],
      "metadata": {
        "id": "gRPs5eKO7nkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0Eps0Ou78w9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}