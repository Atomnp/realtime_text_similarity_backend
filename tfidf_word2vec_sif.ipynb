{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfidf_word2vec_sif.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atomnp/realtime_text_similarity_backend/blob/main/tfidf_word2vec_sif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "CxExVdm3LZrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeRbR-2NAvAS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import coo_matrix, lil_matrix\n",
        "import numpy as np\n",
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wysvPmYtEjY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2829b1-8147-4ccc-f07a-c2546f45cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=[\"?\",\"n't\",\"'s\"]\n",
        "stop_words+=nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "CkcMya7t8CA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting your google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xloBLoD9FfKw",
        "outputId": "3626422e-83b7-4761-8618-622dc3e73c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the data"
      ],
      "metadata": {
        "id": "3KOwZpxvIimd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make shortcut of [this](https://drive.google.com/drive/folders/1BGr0cWKiJwT_jNg9nRNAhWgy0mYPgw_K?usp=sharing) folder in your gdrive**"
      ],
      "metadata": {
        "id": "cJnXuBlZG23A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = r'/gdrive/MyDrive/minor_project_files/filtered.txt'\n",
        "\n",
        "# df= pd.read_csv(dataset, keep_default_na=False, na_values=['_'])\n",
        "questions = []\n",
        "with open(dataset,\"r\") as fp:\n",
        "  questions=[x.strip().lower() for x in fp.readlines()]"
      ],
      "metadata": {
        "id": "42wJ0pffSiRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize sentences\n",
        "sentences = (word_tokenize(sentence) for sentence in questions)"
      ],
      "metadata": {
        "id": "ysCGVpAeOUbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec Model (Train, Save and Load)"
      ],
      "metadata": {
        "id": "mWALbqWcFBtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "class callback(CallbackAny2Vec):\n",
        "    '''Callback to print loss after each epoch.'''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "      loss = model.get_latest_training_loss()\n",
        "      if self.epoch == 0:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
        "      else:\n",
        "          print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
        "      self.epoch += 1\n",
        "      self.loss_previous_step = loss"
      ],
      "metadata": {
        "id": "2sNEI7X9M3di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment if you want to retrain the word2vec model\n",
        "\n",
        "it_copy, sentences = itertools.tee(sentences)\n",
        "\n",
        "# min_count, window=3?? !!\n",
        "model = Word2Vec(sentences=list(it_copy), size=100, window=5, min_count=1, workers=4, compute_loss=True, iter=6, callbacks=[callback()])\n",
        "model_wv = model.wv\n",
        "model.save(\"/gdrive/MyDrive/minor_project_files/word2vec.model\")\n",
        "# # model = FastText( window=3, min_count=1)  # instantiate\n",
        "# # model.build_vocab(sentences=it_copy)\n",
        "# # model.train(sentences=it_copy, total_examples=len(questions), epochs=10)  # train\n",
        "# # model.save(\"/gdrive/MyDrive/minor_project_files/fasttext.model\")"
      ],
      "metadata": {
        "id": "GDD83pHX8FaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7518b50-12be-4af6-a93c-6bd79f32730e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after epoch 0: 1708216.5\n",
            "Loss after epoch 1: 1408493.5\n",
            "Loss after epoch 2: 1206733.0\n",
            "Loss after epoch 3: 1066334.0\n",
            "Loss after epoch 4: 1033058.5\n",
            "Loss after epoch 5: 946525.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load already saved word2vec model\n",
        "# model = FastText.load(\"/gdrive/MyDrive/minor_project_files/fasttext.model\")\n",
        "model = Word2Vec.load(\"/gdrive/MyDrive/minor_project_files/word2vec.model\")\n",
        "model_wv = model.wv"
      ],
      "metadata": {
        "id": "6N1UnBrUORUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('china')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgkzTCEtGQRn",
        "outputId": "c76890db-45de-4af5-bcc3-3a692f5472f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('russia', 0.8251861333847046),\n",
              " ('japan', 0.8227856159210205),\n",
              " ('pakistan', 0.8111938238143921),\n",
              " ('israel', 0.7959840893745422),\n",
              " ('america', 0.7884897589683533),\n",
              " ('taiwan', 0.7727714776992798),\n",
              " ('iran', 0.7588338851928711),\n",
              " ('philippines', 0.7560622096061707),\n",
              " ('africa', 0.7530892491340637),\n",
              " ('turkey', 0.7486889362335205)]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "^^ First train the model on the entire dataset"
      ],
      "metadata": {
        "id": "Y_gDCmUi3T32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Alternative word embedding method: GloVe"
      ],
      "metadata": {
        "id": "E3OSIIv-ENlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GloVe Model embeddings matrix\n",
        "embeddings_index = dict()\n",
        "\n",
        "# reading Glove word embeddings into a dictionary with \"word\" as key and values as word vectors\n",
        "with open('/gdrive/MyDrive/minor_project_files/glove.6B.100d.txt') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "model_wv = embeddings_index"
      ],
      "metadata": {
        "id": "jCpgs5WtDq7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TFIDF for finding important words in a sentence"
      ],
      "metadata": {
        "id": "efMdPLqbHuPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "      return text\n",
        "\n",
        "# lowercase !!\n",
        "vect = TfidfVectorizer(stop_words=stop_words, use_idf=True, tokenizer=identity_tokenizer,lowercase=False)    \n",
        "# copy the iterator so that the cell can be rerun (otherwise the iterator will be at the end)\n",
        "it_copy, sentences = itertools.tee(sentences)\n",
        "tfidf_matrix = vect.fit_transform(it_copy)\n",
        "fv = vect.get_feature_names()"
      ],
      "metadata": {
        "id": "qK8-4oYe2Xb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementation to find sentence embeddings (alternative 1) : lil_matrix! 2m4s!!\n",
        "\n",
        "# cx = coo_matrix(tfidf_matrix)\n",
        "cx = lil_matrix(tfidf_matrix)"
      ],
      "metadata": {
        "id": "HXMKv8ZE49dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find sentence Embeddings for each sentence in the dataset\n",
        "\n",
        "\n",
        "1.   Find most important words\n",
        "2.   Lookup their word vectors from word2vec/glove model\n",
        "3.   Save the average word vector as the sentence embedding\n",
        "\n"
      ],
      "metadata": {
        "id": "ehay95iFH0o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sem = []\n",
        "to = cx.get_shape()[0]\n",
        "for i in range(to):\n",
        "  rx = cx.getrow(i).tocoo()\n",
        "\n",
        "  sorted_by_tfidf = sorted([(fv[j],v) for k,j,v in zip(rx.row, rx.col, rx.data)], key=lambda x: x[1], reverse=True)\n",
        "  if i in [1,500,1000]:\n",
        "    print(sorted_by_tfidf)\n",
        "\n",
        "  sorted_by_tfidf = list(filter(lambda x: x[0] in model_wv, sorted_by_tfidf))\n",
        "  arrlist = np.array( list(map(lambda x: model_wv[x[0]], sorted_by_tfidf[:5])  ))\n",
        "\n",
        "  sem.append(np.mean(arrlist, axis=0))\n",
        "\n",
        "sem = np.asarray(sem)\n",
        "# np.save('/gdrive/MyDrive/minor_project_files/sentence_embeddings4.npy', sem)\n",
        "np.save('/gdrive/MyDrive/minor_project_files/glove_sentence_embeddings4.npy', sem)\n"
      ],
      "metadata": {
        "id": "Hlr9BemrwnRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ecd60ff-88d6-4a46-9b65-75e1fb928ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('care.com', 0.7546053809320284), ('first', 0.35224776891547877), ('job', 0.34440532246877414), ('work', 0.3314647551924048), ('like', 0.2792996925936627)]\n",
            "[('law', 0.6021351916751914), ('civil', 0.5989092139444282), ('examples', 0.5279592450168362)]\n",
            "[('pokémon', 0.5499085913046504), ('yet', 0.5206739448942341), ('working', 0.43720610120089487), ('go', 0.38136197989797543), ('india', 0.2998550477791224)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sem = np.load('/gdrive/MyDrive/minor_project_files/sentence_embeddings4.npy', allow_pickle=True)\n",
        "print(sem[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWkKjayehxkE",
        "outputId": "ecaeabbf-da50-4191-f50a-b95bf90b5c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 0.3117105 ,  0.798547  , -0.3517407 , -0.57282555,  1.2834091 ,\n",
            "       -0.4639574 ,  0.8048502 ,  0.2604505 ,  0.2879932 ,  0.66775215,\n",
            "        1.1243334 ,  0.58522564, -0.22775023, -0.49574035, -0.28327748,\n",
            "        0.08783732, -0.991526  , -0.13409904, -0.7097201 ,  0.31242192,\n",
            "       -0.021375  , -0.21674934,  0.52010095,  0.09773171, -0.39059073,\n",
            "       -0.39624795, -0.52537084,  0.38627592,  0.01615157,  0.11585979,\n",
            "       -0.522874  ,  0.44573665, -0.24741313,  0.67889136,  0.6839901 ,\n",
            "        0.4969235 ,  0.61014324,  0.23550949, -0.08888514,  0.87053615,\n",
            "        0.31962043, -0.8116109 ,  1.0099776 , -0.62597007, -0.06193497,\n",
            "       -0.2162857 ,  1.1613632 , -0.6052805 ,  0.192239  , -0.26050115,\n",
            "        0.6963515 ,  0.22888465, -0.787995  , -1.3464333 ,  0.21761307,\n",
            "       -0.11376771,  0.57417274,  0.03277395,  0.5498637 ,  0.04191905,\n",
            "        0.69846714,  0.66302496, -0.21729198, -0.11927372, -0.31712413,\n",
            "       -0.23745637, -0.34448057, -0.05416854, -0.9802536 ,  1.3934393 ,\n",
            "        0.03405645,  0.5563985 ,  0.28030473, -0.54483646,  0.48220825,\n",
            "       -0.9111072 ,  0.4439342 , -0.46145168, -0.5298645 ,  0.01248553,\n",
            "       -0.30268517, -0.32873434,  0.44840607, -0.6294378 , -0.27222544,\n",
            "       -0.23404686,  0.6992558 ,  0.3464889 , -0.24358189,  0.02567211,\n",
            "       -0.8461145 ,  0.7929677 ,  0.2664073 ,  0.1506469 ,  0.61079407,\n",
            "        0.63886154,  0.09810381, -0.39840457, -0.3908216 , -0.20017453],\n",
            "      dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Annoy Index for finding approximate nearest neighbours (and corresponding label) "
      ],
      "metadata": {
        "id": "l1hhSnW6fbFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7c5VbFcfcZM",
        "outputId": "c5f313dc-de69-4d82-f351-575deac47529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 8.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391683 sha256=24ae1028da02cdd7d8455f102d2cb906a8670278710babb4e29c0890d5125c79\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import annoy\n",
        "import pickle\n",
        "class AnnoyIndex():\n",
        "    def __init__(self,dimension):\n",
        "        self.dimension = dimension\n",
        "        self.index = annoy.AnnoyIndex(self.dimension)   \n",
        "   \n",
        "    def build(self,vectors, labels, number_of_trees=5):\n",
        "        self.vectors = vectors\n",
        "        self.labels = labels \n",
        "\n",
        "        for i, vec in enumerate(self.vectors):\n",
        "          if not np.isnan(np.sum(vec)):\n",
        "            self.index.add_item(i, vec)\n",
        "        self.index.build(number_of_trees)\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.get_nns_by_vector(\n",
        "              list(vector), \n",
        "              k)                                           \n",
        "        return [self.labels[i] for i in indices]\n",
        "    def save(self,path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        print(label_path)\n",
        "        with open(label_path,'wb') as fp:\n",
        "            pickle.dump(self.labels,fp)\n",
        "        self.index.save(path)\n",
        "    \n",
        "    def load(self,path):\n",
        "        label_path=path.split(\".\")[0]+\".labels\"\n",
        "        self.index=annoy.AnnoyIndex(self.dimension)\n",
        "        with open(label_path,\"rb\") as fp:\n",
        "            self.labels=pickle.load(fp)\n",
        "        self.index.load(path)"
      ],
      "metadata": {
        "id": "2WEjwnoBffNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "with open(dataset,\"r\") as fp:\n",
        "  questions=fp.readlines()"
      ],
      "metadata": {
        "id": "OfkGGxp3UGtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create annoy index from vectors\n",
        "index = AnnoyIndex(dimension=len(sem[0]))\n",
        "index.build(sem, questions)"
      ],
      "metadata": {
        "id": "hdujOv9ufneE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index.save('/gdrive/MyDrive/minor_project_files/annoy_index.ann')\n",
        "# index.save('/gdrive/MyDrive/minor_project_files/annoy_index_glove.ann')"
      ],
      "metadata": {
        "id": "vEM0Zby5OtJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3b63d6-ac4d-4970-b6e5-f190409fc003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/gdrive/MyDrive/minor_project_files/annoy_index_glove.labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.query(sem[500])\n",
        "# print(questions[1])\n",
        "# print(sem[1])"
      ],
      "metadata": {
        "id": "GE085Mn0ft30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51eb86b8-9ffa-4180-d28c-31950556b01a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What are the civil law examples?\\n',\n",
              " 'What are civil laws and what are some examples?\\n',\n",
              " 'Why is it so difficult to find basic information about Indian civil law over the internet?\\n',\n",
              " 'What is civil disobedience in law?\\n',\n",
              " 'What is the difference between legal and law?\\n',\n",
              " 'What are examples of law of demand?\\n',\n",
              " 'What is an example of the Law of Conservation of Matter?\\n',\n",
              " 'What are some examples of integrity being shown in law enforcement?\\n',\n",
              " 'What is public disclosure law and how it is applied?\\n',\n",
              " 'Why is maritime law so important?\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  load existing annoy index from file\n",
        "loaded_index = AnnoyIndex(dimension=len(sem[0]))\n",
        "loaded_index.load('/gdrive/MyDrive/minor_project_files/annoy_index.ann')"
      ],
      "metadata": {
        "id": "xo2SUQlsOe5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index.query(sem[499])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWBGa3d0G_D7",
        "outputId": "99679aa0-cbcc-4119-a09d-43df9ae55e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Could an extremely advanced civilization, in the far future, deactivate and store red and brown dwarfs to delay heat death?\\n',\n",
              " 'Why do I feel extreme anger and crave revenge for any slight, no matter how insignificant?\\n',\n",
              " 'I have no gyno (tested) yet my nipples are puffy and they look like female breasts. Why?\\n',\n",
              " 'Why do some Orthodox Jewish circumcisions involve the mohel using his mouth to draw blood?\\n',\n",
              " 'Are most women attracted to men with overly masculine faces, overly feminine faces, or in-between? What influences their preference?\\n',\n",
              " \"How do I stop my son who has Asperger's from destroying his bedroom walls and hurting people when he's in a rage?\\n\",\n",
              " 'Do you think the climate of arrogant rudeness afforded by Internet anonymity will ever spill over to the real world?\\n',\n",
              " 'What are the sexiest pornstars?\\n',\n",
              " 'How do spiders mate?\\n',\n",
              " 'Can I commit mental disorder disability fraud at age 30 if I have 240k saved up from my job?\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(input_question:str):\n",
        "    # get sentence embedding of the question\n",
        "    to_transform = word_tokenize(input_question)\n",
        "    matrix = vect.transform([to_transform])\n",
        "    # print(matrix)\n",
        "    cx = coo_matrix(matrix)\n",
        "\n",
        "    sorted_by_tfidf = sorted([(fv[j],v) for i,j,v in zip(cx.row, cx.col, cx.data)], key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    # print(sorted_by_tfidf)\n",
        "    filtered = list(filter(lambda x: x[0] in model_wv, sorted_by_tfidf))\n",
        "\n",
        "    arrlist = np.array( list(map(lambda x: model_wv[x[0]], sorted_by_tfidf[:5])  ))\n",
        "\n",
        "    sentence_embedding = np.mean(arrlist, axis=0)\n",
        "    # print(sentence_embedding)\n",
        "    return index.query(sentence_embedding)"
      ],
      "metadata": {
        "id": "c53z2LkESBIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title { run: \"auto\" }\n",
        "query = \"Should I buy the new macbook?\" #@param {type:\"string\"}\n",
        "\n",
        "print(\"Finding relevant items in the index...\")\n",
        "print(get_similar(query))\n",
        "# %time query_embedding = get_similar(query)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oX_5ln1YB9W",
        "outputId": "16c14ace-4ff8-438c-b406-fa808f5ce52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finding relevant items in the index...\n",
            "['Should I buy the new MacBook?\\n', 'What is the cheapest way to buy a MacBook Pro?\\n', 'How can I buy the new Macbook 12\" M7 512GB model in India?\\n', 'Should I buy a Macbook or a Macbook Pro?\\n', 'Is it ok to buy a MacBook Air from Amazon?\\n', 'Should I buy the new iPhone 7?\\n', 'Should I buy the new MacBook 2016 or one from 2015?\\n', 'Is a MacBook really worth buying more than a PC?\\n', 'What are the cons of buying a refurbished MacBook Air or MacBook Pro?\\n', 'Would you buy an iPod nano or iPod touch? Which one is more worth it?\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running Flask on collab"
      ],
      "metadata": {
        "id": "CfEnbd3JTV-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!sudo apt update && sudo apt install ngrok\n",
        "!pip install flask_ngrok flask-bootstrap\n",
        "!pip install flask_restful flask_cors\n",
        "!cat /gdrive/MyDrive/minor_project_files/ngrok_token | xargs ngrok authtoken"
      ],
      "metadata": {
        "id": "moNKtS4sXFgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "stdout = sys.stdout\n",
        "stderr = sys.stderr"
      ],
      "metadata": {
        "id": "4L4DjSwU-Avm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGs-FuZu_O7z",
        "outputId": "fbe3af98-d1fc-48a9-84c5-0dd8d3d25fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, render_template , request , jsonify\n",
        "from flask_restful import Resource, Api\n",
        "import os, logging, sys\n",
        "from flask_cors import CORS, cross_origin\n",
        "\n",
        "# sys.stdout = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"w\", buffering=1)\n",
        "# sys.stderr = open(\"/gdrive/MyDrive/minor_project_files/test.txt\", \"a\", buffering=1)\n",
        "\n",
        "app = Flask(__name__)\n",
        "cors = CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "# cors = CORS(app)\n",
        "# app.config['CORS_HEADERS'] = 'Content-Type'\n",
        "api = Api(app)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "\n",
        "class Similarity(Resource):\n",
        "  # get endpoint to check server is up\n",
        "    def get(self):\n",
        "        return jsonify({\"hello\": \"Server Online!\"})\n",
        "\n",
        "    def post(self):\n",
        "        json_data = request.get_json(force=True)\n",
        "        qn = json_data[\"question\"]\n",
        "        # similarity = get_similar(qn)\n",
        "        # return list of questions\n",
        "        x = [\"question 1\", \"question 2\"]\n",
        "        return x\n",
        "\n",
        "\n",
        "api.add_resource(Similarity, \"/\")\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp3oi91wTVVI",
        "outputId": "ea23714d-7849-44f4-9e97-cf7dff2d648e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://a520-35-227-155-192.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [02/Feb/2022 15:20:48] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [02/Feb/2022 15:20:49] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.stdout = stdout\n",
        "sys.stderr = stderr"
      ],
      "metadata": {
        "id": "gRPs5eKO7nkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P0Eps0Ou78w9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}